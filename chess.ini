[base]
package = ocean
env_name = puffer_chess
policy_name = ChessCNN
rnn_name = Recurrent

[env]
num_envs = 1
max_moves = 3000
opponent_depth = -1
reward_shaping_weight = 0.0
reward_draw = -0.5
reward_invalid_piece = -0.1
reward_invalid_move = -0.1
reward_valid_piece = 0.01
reward_valid_move = 0.05
render_fps = 5
human_play = 0
log_interval = 2048
starting_fen = rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1

[vec]
num_envs = 12288
num_workers = 12
batch_size = 12288
backend = Multiprocessing
zero_copy = True

[train]
device = cuda
total_timesteps = 50000000000
batch_size = 786432
minibatch_size = 65536
bptt_horizon = 64
learning_rate = 0.0003
gamma = 0.998
gae_lambda = 0.9176902211512185
clip_coef = 0.2
vf_coef = 1.5778154267738698
vf_clip_coef = 0.1
adam_beta1 = 0.9
adam_beta2 = 0.999
adam_eps = 1e-8
max_grad_norm = 1.7827046449434
optimizer = muon
precision = bfloat16
update_epochs = 1
ent_coef = 0.01
vtrace_rho_clip = 1.0
vtrace_c_clip = 1.0
prio_alpha = 0.6
prio_beta0 = 0.4
checkpoint_interval = 200
compile = False
compile_mode = default

[policy]
hidden_size = 256

[rnn]
input_size = 256
hidden_size = 256

[sweep]
metric = winrate

[sweep.train.total_timesteps]
distribution = log_normal
min = 1e9
max = 1e11
mean = 5e10
scale = 0.5

[sweep.env.reward_draw]
distribution = uniform
min = -1.0
max = 0.0
mean = -0.5
scale = auto

[sweep.env.reward_invalid_piece]
distribution = uniform
min = -0.5
max = 0.0
mean = -0.1
scale = auto

[sweep.env.reward_invalid_move]
distribution = uniform
min = -0.5
max = 0.0
mean = -0.1
scale = auto

[sweep.env.reward_valid_piece]
distribution = uniform
min = 0.0
max = 0.1
mean = 0.01
scale = auto

[sweep.env.reward_valid_move]
distribution = uniform
min = 0.0
max = 0.1
mean = 0.05
scale = auto
