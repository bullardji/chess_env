[base]
package = ocean
env_name = puffer_chess
policy_name = ChessCNN
rnn_name = Recurrent
max_suggestion_cost = 300000000

[env]
num_envs = 4096
max_moves = 50000 #keeping very high to avoid timeouts. Practically should never be reached.
opponent_depth = 1
reward_shaping_weight = 0 #0.13189766946566
reward_draw = 0
reward_invalid_piece = -0.1
reward_invalid_move = -0.1
reward_valid_piece = 0.0
reward_valid_move = 0.0
log_interval = 16
render_fps = 30
human_play = 0
starting_fen = rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1 

[vec]
num_envs = 8
backend = Multiprocessing

[train]
device = cuda
total_timesteps = 1_000_000_000
max_runs = 100
learning_rate = 0.01278248060326187
gamma = 0.980588940416519
gae_lambda = 0.5999999999999999
clip_coef = 0.5395414850844464
vf_coef = 3.408777911131735
vf_clip_coef = 4.239154954113652
max_grad_norm = 2.1423060072887696
ent_coef = 0.06272671231080278
adam_beta1 = 0.901551117863987
adam_beta2 = 0.9989872212166014
adam_eps = 4.723325553958992e-10
optimizer = muon
precision = float32
minibatch_size = 32768
bptt_horizon = 64
checkpoint_interval = 200

[policy]
hidden_size = 256
noise_epsilon = 0.25
noise_alpha = 0.3

[rnn]
input_size = 256
hidden_size = 256

[sweep]
method = Protein
metric = score
goal = maximize
downsample = 5
use_gpu = True
prune_pareto = True

[sweep.train.total_timesteps]
distribution = uniform
min = 500_000_000
max = 1_000_000_000
mean = 750_000_000
scale = auto

[sweep.env.reward_shaping_weight]
distribution = uniform
min = 0.0
max = 0.2
mean = 0.05
scale = auto

[sweep.train.ent_coef]
distribution = log_normal
min = 0.00001
mean = 0.01
max = 0.2
scale = auto

[sweep.train.learning_rate]
distribution = log_normal
min = 0.00001
mean = 0.01
max = 0.1
scale = 0.5

[sweep.train.max_grad_norm]
distribution = uniform
min = 0.0
mean = 1.0
max = 5.0
scale = auto
